{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continous Control Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrey Naumov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the DDPG learning algorithm based on the Continous control with deep reinforcement learning paper (Lillicrap et al., 2016). The starting code was based on the Bipendulum implementation introduced during the class.\n",
    "\n",
    "DDPG is an actor-critic method where the Critic learns from the value function and it determines how the Actor policy improves. To decrease the instability of the model, I used a replay buffer and a soft target update.\n",
    "\n",
    "DDPG algorithm continuously improves the policy while exploring the environment and\n",
    "converges on large action space by using the actor-critic architecture. The actor specifies action\n",
    "in a current state while critic criticizes the actions made by the actor by using Temporal\n",
    "Difference Error.\n",
    "\n",
    "It maximizes the action-value function to compute the direction to change the current action to\n",
    "increase overall discounted reward. However, it does not take into consideration how\n",
    "exploration is done. In the implemented DDPG code an agent adds its experience to the replay\n",
    "buffer and local actor and critic are updated 10 times in a row using different samples from the\n",
    "replay buffer. The OUNoise parameters are also experimented to add noise to the action space\n",
    "of the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actor model is a neural network with one hidden layer with size 256. I used ReLU as the activation function and tanh is used in the final layer to return the action output.\n",
    "\n",
    "The critic model is a neural network with three hidden layers of size 256, 256 and 128. I used ReLU as the activation function and linear is used in the final layer to return the action output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  BUFFER_SIZE = int(1e6)\n",
    "-  BATCH_SIZE = 128  \n",
    "-  GAMMA = 0.99\n",
    "-  TAU = 1e-3\n",
    "-  LR_ACTOR = 1e-4\n",
    "-  LR_CRITIC = 3e-4\n",
    "-  WEIGHT_DECAY = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I solved the environment in 165 episodes of total 300. Scores are shown on fig. 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episode 100\tAverage Score: 11.94\n",
    "<br>\n",
    "Episode 165\tAverage Score: 30.26\n",
    "<br>\n",
    "**Environment solved in 165 episodes!\tAverage Score: 30.26**\n",
    "<br>\n",
    "Episode 200\tAverage Score: 41.81\n",
    "<br>\n",
    "Episode 300\tAverage Score: 75.02\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1:![](result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Optimize the parameters\n",
    "-  Experiment with other off-policy reinforcement learning algorithms and develop a\n",
    "strong understanding\n",
    "-  Solve Crawl challenge which is a more difficult control continuous environment\n",
    "-  Try version of environment with 20 agents "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
